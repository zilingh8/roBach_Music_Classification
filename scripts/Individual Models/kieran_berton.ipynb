{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from collections import Counter\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import consolidated saved csvs of wav features\n",
    "\n",
    "train_wav = pd.read_csv('../data/df_train_wav_consolidated.csv',index_col=0)\n",
    "test_wav = pd.read_csv('../data/df_test_wav_finc.csv',index_col=0)\n",
    "\n",
    "#Get metadata\n",
    "metadata=pd.read_csv('../data/musicnet_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy of the metadata\n",
    "meta_data_copy = metadata.copy(deep=True)\n",
    "meta_data_copy.reset_index(inplace=True)\n",
    "#Rename column name\n",
    "meta_data_copy = meta_data_copy.rename(columns = {'id':'filename'})\n",
    "\n",
    "merged_train_data = pd.merge(train_wav , meta_data_copy , on=\"filename\")\n",
    "merged_train_data = merged_train_data.drop([\"composer\", \"composition\", \"movement\",\"source\",\"transcriber\",\"catalog_name\",\"index\"], axis=1)\n",
    "\n",
    "merged_test_data = pd.merge(test_wav , meta_data_copy , on=\"filename\")\n",
    "merged_test_data = merged_test_data.drop([\"composer\", \"composition\", \"movement\",\"source\",\"transcriber\",\"catalog_name\",\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import midi file tables\n",
    "# train_labels = pd.read_csv('../data/df_train_labels_consol.csv')\n",
    "# test_labels = pd.read_csv('../data/df_test_labels_consol.csv')\n",
    "\n",
    "# #combine train and test data\n",
    "# labels_frames = [train_labels,test_labels]\n",
    "# labels = pd.concat(labels_frames , ignore_index=True)\n",
    "\n",
    "# #create series with various midi features\n",
    "\n",
    "# #number of unique instruments\n",
    "# midi_nunique_inst = labels.groupby('file_name').agg({\"instrument\": \"nunique\"}).reset_index().rename(columns={\"instrument\": \"midi_nunique_inst\"})\n",
    "\n",
    "# #number of unique notes\n",
    "# midi_nunique_note = labels.groupby('file_name').agg({\"note\": \"nunique\"}).reset_index().rename(columns={\"note\": \"midi_nunique_note\"})\n",
    "\n",
    "# #total number of notes\n",
    "# midi_num_notes = labels.groupby('file_name').sum()[['note']].reset_index().rename(columns={'note':'midi_num_notes'})\n",
    "\n",
    "# #quintiles of the distribution of note values (pitches)\n",
    "# midi_min_note = labels.groupby('file_name').min()[['note']].reset_index().rename(columns={'note':'midi_min_note'})\n",
    "# midi_second_quintile_note = labels[['file_name','note']].groupby('file_name').quantile(q=0.25,interpolation='nearest')[['note']].reset_index().rename(columns={'note':'midi_second_quintile_note'})\n",
    "# midi_median_note = labels[['file_name','note']].groupby('file_name').quantile(interpolation='nearest')[['note']].reset_index().rename(columns={'note':'midi_median_note'})\n",
    "# midi_fourth_quintile_note = labels[['file_name','note']].groupby('file_name').quantile(q=0.75,interpolation='nearest')[['note']].reset_index().rename(columns={'note':'midi_fourth_quintile_note'})\n",
    "# midi_max_note = labels[['file_name','note']].groupby('file_name').max()[['note']].reset_index().rename(columns={'note':'midi_max_note'})\n",
    "\n",
    "# #average number of notes per instrument\n",
    "# midi_avg_notes_inst = labels.groupby(['file_name','instrument']).sum()[['note']].groupby('file_name').mean().reset_index().rename(columns={'note':'midi_avg_notes_inst'})\n",
    "\n",
    "# #merge all columns together\n",
    "# midi_features = midi_nunique_inst.merge(midi_nunique_note, on='file_name')\n",
    "# midi_features = midi_features.merge(midi_num_notes,on='file_name')\n",
    "# midi_features = midi_features.merge(midi_min_note,on='file_name')\n",
    "# midi_features = midi_features.merge(midi_second_quintile_note,on='file_name')\n",
    "# midi_features = midi_features.merge(midi_median_note,on='file_name')\n",
    "# midi_features = midi_features.merge(midi_fourth_quintile_note,on='file_name')\n",
    "# midi_features = midi_features.merge(midi_max_note,on='file_name')\n",
    "# midi_features = midi_features.merge(midi_avg_notes_inst,on='file_name')\n",
    "\n",
    "# midi_features.to_csv('../data/midi_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy of the metadata\n",
    "meta_data_copy = metadata.copy(deep=True)\n",
    "meta_data_copy.reset_index(inplace=True)\n",
    "#Rename column name\n",
    "meta_data_copy = meta_data_copy.rename(columns = {'id':'filename'})\n",
    "\n",
    "merged_train_data = pd.merge(train_wav , meta_data_copy , on=\"filename\")\n",
    "merged_train_data = merged_train_data.drop([\"composer\", \"composition\", \"movement\",\"source\",\"transcriber\",\"catalog_name\",\"index\"], axis=1)\n",
    "\n",
    "merged_test_data = pd.merge(test_wav , meta_data_copy , on=\"filename\")\n",
    "merged_test_data = merged_test_data.drop([\"composer\", \"composition\", \"movement\",\"source\",\"transcriber\",\"catalog_name\",\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset of dataset for only max 2 instruments\n",
    "\n",
    "subset_list= ['Accompanied Cello','Accompanied Clarinet','Accompanied Violin','Solo Cello', 'Solo Flute', 'Solo Piano', 'Solo Violin', 'Violin and Harpsichord']\n",
    "subset_merged_train_data = merged_train_data.loc[merged_train_data['ensemble'].isin(subset_list)]\n",
    "subset_merged_test_data = merged_test_data.loc[merged_test_data['ensemble'].isin(subset_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Piano Quintet', 'Solo Piano', 'Piano Trio', 'Viola Quintet',\n",
       "       'String Quartet', 'Clarinet Quintet',\n",
       "       'Pairs Clarinet-Horn-Bassoon', 'Wind Quintet', 'Accompanied Cello',\n",
       "       'Accompanied Clarinet', 'Wind and Strings Octet', 'String Sextet',\n",
       "       'Piano Quartet', 'Horn Piano Trio', 'Solo Violin', 'Solo Flute',\n",
       "       'Solo Cello', 'Violin and Harpsichord',\n",
       "       'Clarinet-Cello-Piano Trio', 'Accompanied Violin', 'Wind Octet'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of unique ensembles\n",
    "ens_list = merged_train_data['ensemble'].unique()\n",
    "ens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Kieran_External_HD/Users/Kieran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Volumes/Kieran_External_HD/Users/Kieran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Map list of unique ensemble names to integer\n",
    "mapping = {item:i for i, item in enumerate(ens_list)}\n",
    "\n",
    "merged_train_data[\"ensemble\"] = merged_train_data[\"ensemble\"].apply(lambda x: mapping[x])\n",
    "subset_merged_train_data[\"ensemble\"] = subset_merged_train_data[\"ensemble\"].apply(lambda x: mapping[x])\n",
    "\n",
    "merged_test_data[\"ensemble\"] = merged_test_data[\"ensemble\"].apply(lambda x: mapping[x])\n",
    "subset_merged_test_data[\"ensemble\"] = subset_merged_test_data[\"ensemble\"].apply(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the original train test split given in kaggle\n",
    "X_original_train = merged_train_data.iloc[:,np.r_[:167,168]]\n",
    "X_original_test = merged_test_data.iloc[:,np.r_[:167,168]]\n",
    "\n",
    "y_original_train = merged_train_data.iloc[:,167:168]\n",
    "y_original_test = merged_test_data.iloc[:,167:168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the training and test data\n",
    "x_frames = [X_original_train,X_original_test]\n",
    "X = pd.concat(x_frames , ignore_index=True)\n",
    "\n",
    "y_frames = [y_original_train,y_original_test]\n",
    "y = pd.concat(y_frames , ignore_index=True)\n",
    "\n",
    "#Find the index position of viola quintet and drop it since there is only one ensemble of that type\n",
    "index_violaquintet = y[ y['ensemble'] == 3 ].index\n",
    "\n",
    "y.drop(index_violaquintet , inplace=True)\n",
    "X.drop(index_violaquintet , inplace=True)\n",
    "\n",
    "###########################################ADDING MIDI FEATURES################################\n",
    "#import and add midi features to input data\n",
    "midi_features = pd.read_csv('../data/midi_features.csv')\n",
    "midi_features.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "X_with_midi = X.merge(midi_features,left_on = 'filename',right_on='file_name')\n",
    "X_with_midi.drop('filename',axis=1,inplace=True)\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, stratify=y, random_state=101)\n",
    "X_train_with_midi, X_test_with_midi, y_train, y_test = train_test_split(X_with_midi, y, train_size=0.80, test_size=0.20, stratify=y, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_with_midi.to_csv('../data/df_wav_and_midi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('filename',axis=1,inplace=True) \n",
    "X_test.drop('filename',axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all possible values of principal components can go up to the number of features\n",
    "all_possible_k = range(1,len(X_train.columns))\n",
    "\n",
    "# Create empty list to store fraction of total variance\n",
    "all_possible_k_variance = []\n",
    "\n",
    "# Write for loop to try k values for PCA\n",
    "for i in all_possible_k:\n",
    "    pca=PCA(n_components = i)\n",
    "    pca.fit(X_train)\n",
    "    train_pca = pca.transform(X_train)\n",
    "    test_pca = pca.transform(X_test)\n",
    "    \n",
    "    #Append fractions of explained total variance\n",
    "    all_possible_k_variance.append(np.sum(pca.explained_variance_ratio_))\n",
    "    \n",
    "    #Logistic Regression Model with PCA\n",
    "    reg = linear_model.LogisticRegression(max_iter = 100,solver='liblinear')\n",
    "    reg.fit(train_pca,y_train)\n",
    "    reg_predict = reg.predict(test_pca)\n",
    "    print(\"F1 Score with \", i,\" principal components: {:.2f}\".format(f1_score(y_test,reg_predict,average='weighted')))\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(all_possible_k[0:8], all_possible_k_variance[0:8])\n",
    "print(all_possible_k_variance[0:8])\n",
    "\n",
    "# show lineplot of fraction of total variance vs. number of principal components, for all possible numbers of principal components.\n",
    "# Add title and axis names\n",
    "plt.title('Number of PCA Components vs Explained Variance Fraction ')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components = 5)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(linear_model.LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model with PCA\n",
    "reg = linear_model.LogisticRegression(max_iter = 100,solver='liblinear')\n",
    "reg.fit(X_train_pca,y_train)\n",
    "reg_predict = reg.predict(X_test_pca)\n",
    "print(classification_report(y_test,reg_predict))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# # Plot non-normalized confusion matrix\n",
    "# titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "#                   (\"Normalized confusion matrix\", 'true')]\n",
    "# for title, normalize in titles_options:\n",
    "#     disp = plot_confusion_matrix(reg, X_test_pca, y_test,\n",
    "#                                 #  display_labels=ens_list,\n",
    "#                                  cmap=plt.cm.Blues,\n",
    "#                                  normalize=normalize)\n",
    "#     disp.ax_.set_title(title)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model without PCA\n",
    "reg = linear_model.LogisticRegression(max_iter = 100,solver='liblinear')\n",
    "reg.fit(X_train,y_train)\n",
    "reg_predict = reg.predict(X_test)\n",
    "print(classification_report(y_test,reg_predict))\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
